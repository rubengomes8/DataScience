##############################################################
#1 https://andriymz.github.io/certifications/crt020-feedback/#
##############################################################

1. identify actions and transformations

R.: Transformations are Spark operation which will transform one RDD into another. Transformations will always create new RDD from original one. 

Some basic transformations:
- map
- flatMap
- filter
- groupByKey
- reduceByKey
- sample
- union
- distinct

Some basic actions:
- collect
- take n
- count
- max
- min
- sum
- variance
- stdev
- reduce

2. differentiate between narrow and wide transformations

- narrow: each partition of the parent RDD is used by at most one partition of the child RDD

	- map
	- filter
	- union
	- join w/ inputs co-partitioned

- wide: multiple child RDD partitions may depend on a single parent RDD partition

	- groupByKey
	- join w/ inputs not co-partitioned

3. know how caching works and Spark’s LRU partition eviction

know how to increase or decrease the partition number, with or without shuffling

reason about the pros and cons, GC, execution time and OOM errors when there is data skew in multiple
 setups, e.g it is better to have a setup with 1 executor with 100GB of RAM and 20 cores or 10 executors with 10GB of RAM and 2 cores each, etc.

know how shuffle works

know what is a DAG and Spark’s units of execution: Job, Stage and Task

know the relation between the RDD partition count and the number of spawned Tasks

Spark Architecture Components
Candidates are expected to be familiar with the following architectural components and their relationship to each other:
Driver
Executor
Cores/Slots/Threads
Partitions

Spark Execution
Candidates are expected to be familiar with Spark’s execution model and the breakdown between the different elements:
Jobs
Stages
Tasks

Spark Concepts
Candidates are expected to be familiar with the following concepts:
Caching
Shuffling
Partitioning
Wide vs. Narrow Transformations
DataFrame Transformations vs. Actions vs. Operations
High-level Cluster Configuration

SparkSession
Candidates are expected to know how to:
Create a DataFrame/Dataset from a collection (e.g. list or set)
Create a DataFrame for a range of numbers
Access the DataFrameReaders
Register User Defined Functions - UDFs

DataFrameReader
Candidates are expected to know how to:
Read data for the “core” data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)
How to configure options for specific formats
How to specify a DDL-formatted schema
How to construct and specify a schema using the StructType classes

DataFrameWriter
Candidates are expected to know how to:
Write data to the “core” data formats (csv, json, jdbc, orc, parquet, text and tables)
Overwriting existing files
How to configure options for specific formats
How to write a data source to 1 single file or N separate files
How to write partitioned data

DataFrame
Have a working understanding of every action such as take(), collect(), and foreach()
Have a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing, performing joins and unions as well as producing aggregates.
Know how to cache data, specifically to disk, memory or both
Know how to uncache previously cached data
Converting a DataFrame to a global or temp view
Applying hints

Spark SQL Functions
When instructed what to do, candidates are expected to be able to employ the multitude of Spark SQL functions. Examples include, but are not limited to:
Aggregate functions: getting the first or last item from an array or computing the min and max values of a column.
Collection functions: testing if an array contains a value, exploding or flattening data.
Date time functions: parsing strings into timestamps or formatting timestamps into strings
Math functions: computing the cosign, floor or log of a number
Misc functions: converting a value to crc32, md5, sha1 or sha2
Non-aggregate functions: creating an array, testing if a column is null, not-null, nan, etc
Sorting functions: sorting data in descending order, ascending order, and sorting with proper null handling
String functions: applying a provided regular expression, trimming string and extracting substrings.
UDF functions: employing a UDF function.
Window functions: computing the rank or dense rank.

Other
Working with missing data in DataFrames types: by using the DataFrameNaFunctions class
File system utility dbutils: see Databricks Utilities
Convert DataFrame to JSON using the Dataset’s toJSON method
Manage Databases and Tables
Manage Databases and Tables


#################################################################################
# AQE - https://medium.com/swlh/spark-sql-adaptive-query-execution-3adc68973c91 #
#################################################################################

Adaptive Query Execution is an enhancement enabling Spark 3 (officially released just a few days ago) to alter physical execution plans at runtime, which allows improvements on the physical implementation based on statistics collected after shuffle exchange operations.

Level of parallelism and join implementation strategy have been shown to be key factors for query performance on large clusters. According to this premise, new optimization rules have been built on top of the AQE feature:

1. dynamically coalescing shuffle partitions by eventually merging small adjacent ones.

2. dynamically replacing a Sort-Merge join for a Broadcast-Hash join when size conditions are met on any side.

3. dynamically optimizing skew joins by eventually scheduling multiple reduce tasks to work on a single huge partition.
It is expected that future Spark releases will ship with additional optimization rules built on Adaptive Query Execution.

AQE is disable by default.
spark.sql.adaptive.enabled = false

spark.sql.shuffle.partitions = 200  (Spark 2.x)
spark.sql.adaptive.coalescePartitions.enabled = true
spark.sql.adaptive.coalescePartitions.minPartitionNum
spark.sql.adaptive.coalescePartitions.initialPartitionNum = 200
spark.sql.adaptive.advisoryPartitionSizeInBytes = 64MB

BROADCAST JOIN 
spark.sql.autoBroadcastJoinThreshold = 10MB  (Spark 2.x)
spark.sql.adaptive.localShuffleReader.enabled

SKEWED JOIN
spark.sql.adaptive.skewJoin.enabled = true
spark.sql.adaptive.skewJoin.skewedPartitionFactor = 10
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes = 256MB
spark.sql.adaptive.localShuffleReader.enabled














